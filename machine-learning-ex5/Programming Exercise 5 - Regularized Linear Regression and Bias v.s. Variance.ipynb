{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last updated on 7 August 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Regression\n",
    "\n",
    "In the first half of this exercise, we implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir.\n",
    "\n",
    "The dataset has already been divided into three parts for our convenience, comprising of:\n",
    "\n",
    "- A training set that your model will learn on: X, y\n",
    "- A cross validation set for determining the regularization parameter:\n",
    "Xval, yval\n",
    "- A test set for evaluating performance. These are “unseen” examples\n",
    "which your model did not see during training: Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sio.loadmat(\"ex5data1.mat\")\n",
    "X = data[\"X\"].flatten(); y = data[\"y\"].flatten()\n",
    "Xval = data[\"Xval\"].flatten(); yval = data[\"yval\"].flatten()\n",
    "Xtest = data[\"Xtest\"].flatten(); ytest = data[\"ytest\"].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are column vectors shaped as 12 by 1 two dimensional numpy arrays, so it is necessary to flatten them to a 1D numpy array to facilitate easy plotting and subsequent numerical computation.\n",
    "\n",
    "## Visualizing the dataset\n",
    "\n",
    "We visualize all three sets in the same figure. To view a set in isolation, double click on the name of that particular set in the legend at the top right hand corner, and the other two sets will be temporarily removed from the figure. To bring them back, double click on that same set's name again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"b74567c2-8b48-47fc-bb9d-c91c7f9dc65f\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"b74567c2-8b48-47fc-bb9d-c91c7f9dc65f\", [{\"type\": \"scatter\", \"y\": [2.1343105067296686, 1.1732566787564553, 34.35910918053895, 36.83795516371235, 2.808965074479856, 2.121072476666392, 14.710268306562307, 2.614184386432259, 3.7401716656949393, 3.731691310543067, 7.627658852038035, 22.752428302242212], \"x\": [-15.93675813378541, -29.152979217238133, 36.18954862666253, 37.49218733199513, -48.058829452570066, -8.941457938049755, 15.307792889226079, -34.70626581132249, 1.3891543686358903, -44.38375985168692, 7.013502082404112, 22.762748919711303], \"mode\": \"markers\", \"name\": \"Training\"}, {\"type\": \"scatter\", \"y\": [4.170202008850627, 4.067262803838973, 31.873067575789328, 10.623656189695145, 31.836021281338372, 4.959369720997111, 4.451598803470352, 22.27631845748984, -4.387382739157175e-05, 20.503801579308035, 3.8583447626973317, 19.36505291560758, 4.883762805453054, 11.097158848382119, 7.461708266154005, 1.4769346422983864, 2.719163877647038, 10.92690065536873, 8.348712346301108, 52.781927979043786, 13.357339606074687], \"x\": [-16.74653577802069, -14.577470749241868, 34.51575865729932, -47.01007574320076, 36.975119046362785, -40.686110015367476, -4.472010975766456, 26.53363489478886, -42.797683100179555, 25.374099383527664, -31.10955397730775, 27.31176863521341, -3.2638620136567207, -1.8182764871158748, -40.71966240251616, -50.01324364549543, -17.41177154801638, 3.5881936966441286, 7.085480261970673, 46.28236901853893, 14.612289091656539], \"mode\": \"markers\", \"name\": \"Cross Validation\"}, {\"type\": \"scatter\", \"y\": [3.3168895317563605, 5.397689520216154, 0.1304298374520143, 6.192598202383873, 17.088487115476173, 0.7995080467101721, 2.824791834378351, 28.62123333554547, 17.046390806327768, 55.38437334220786, 4.079367333128051, 8.270397934583032, 31.323551019505633, 39.1590610329276, 8.087279893552164, 24.1113438935375, 2.477354802758797, 6.5660647193030375, 6.0380888015720355, 4.692739558592499, 10.830046063205728], \"x\": [-33.318003990618394, -37.91216402831621, -51.206937948273165, -6.1325958482286245, 21.26118327389843, -40.319529490308526, -14.541531672952836, 32.55976024170859, 13.393432547328858, 44.20988594678167, -1.1426776754879553, -12.76686065227885, 34.054505389596635, 39.223500277403794, 1.9744967392389405, 29.62175509870447, -23.669629706659677, -9.011801392661441, -55.94057090677502, -35.70859751906493, 9.510205326576553], \"mode\": \"markers\", \"name\": \"Test\"}], {\"title\": \"Scatter plot of training data\", \"xaxis\": {\"title\": \"Change in water level (x)\"}, \"yaxis\": {\"title\": \"Water flowing out of the dam (y)\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trgdat = go.Scatter(x=X, y=y, mode=\"markers\", name = \"Training\")\n",
    "valdat = go.Scatter(x=Xval, y=yval, mode=\"markers\", name = \"Cross Validation\")\n",
    "testdat = go.Scatter(x=Xtest, y=ytest, mode=\"markers\", name = \"Test\")\n",
    "plotly.offline.iplot({\"data\": [trgdat,valdat,testdat],\n",
    "                      \"layout\": go.Layout(title = \"Scatter plot of training data\",\n",
    "                                          xaxis = dict(title=\"Change in water level (x)\"),\n",
    "                                          yaxis = dict(title=\"Water flowing out of the dam (y)\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_cost(_theta, _X, _y, _lda):\n",
    "    regterm = _lda/(2*_y.size) * np.sum(np.square(_theta))\n",
    "    return 1/(2*_y.size) * np.sum(np.square(_X@_theta - _y)) + regterm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation notes for `add_bias_term()` function\n",
    "\n",
    "Function that accepts an input data matrix and returns as output a design matrix by adding a column of ones to the original data matrix..\n",
    "\n",
    "Note if `_X` is 1D array, performing a transpose on it will not have any effect, but if `_X` is a 2D array (which is the case when `_X` holds polynomial features), then the transpose will take effect to prepare the array to be stacked vertically with the 1D array of ones. Finally a transpose is performed on the stacked 2D array to put it in the correct shape of a design matrix (to suit the linear algebra formulation of a regression).\n",
    "\n",
    "TODO: replace the old implementation of this function in the previous exercises 3 and 4, with this new implementation that uses `np.vstack()` instead of `np.concatenate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_bias_term(_X):\n",
    "    ones_row = np.ones(_X.shape[0])\n",
    "    _X_bias = np.vstack((ones_row, _X.T)).T\n",
    "    return _X_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: vectorization here. explain how this line `(X_bias@Theta - y).reshape((len(y)),1) *X_bias` shows broadcasting in action! The important thing I've learnt about broadcasting is that it works based on matching axes....\n",
    "by reshaping `X_bias@Theta - y` from shape `(12, )` to `(12, 1)`, I turned it into a column which could be broadcasted in the row (horizontal) direction of `X_bias` of shape `(12, 2)`, meaning the `*` operation could be applied to multiply the reshaped `(12,1)` vector to each column of X_bias on the left.\n",
    "At the end, a simple numpy sum along axis 0 gives us the result we need\n",
    "\n",
    "`np.sum((X_bias@Theta - y).reshape((len(y)),1) *X_bias,axis =0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_gradient(_theta,_X,_y,_lda):\n",
    "    regterm = _lda/_y.size * _theta\n",
    "    regterm[0] = 0 # do not regularize the zeroth term\n",
    "    return 1/_y.size * np.sum((_X@_theta - _y).reshape((_y.size),1) * _X,axis =0) + regterm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting linear regression\n",
    "\n",
    "Finally realized that the `scipy.optimize` library contains an `fmin_cg()` routine that directly interfaces with an implementation of the Polack-Ribiere flavor of conjugate gradient optimization, the same flavor of conjugate gradient that was given to us in the MATLAB `fmin_cg.m` file provided by the course. \n",
    "\n",
    "We are using `fmin_cg()` directly here instead of `minimize()` with the passed parameter `method = 'CG'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "def train_linear_regression(_X, _y, _lda):\n",
    "    _theta_optimized = fmin_cg(linear_regression_cost, np.zeros(_X.shape[1]),\n",
    "                              linear_regression_gradient, (_X,_y,_lda), disp=True)\n",
    "    return _theta_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the parameters of our linear model without regularization. At such a low dimension, our hypothesis will not benefit much from regularization so we set $\\lambda=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 22.373906\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n"
     ]
    }
   ],
   "source": [
    "theta_optimized = train_linear_regression(add_bias_term(X), y, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute and visualize our model, observing that it is a poor fit to the data since the data has a non-linear trend that will only be better predicted with a polynomial model, which we will implement later. Before training a polynomial model, we will visualize the learning curves of this linear model to learn how they should look like for such a model that underfits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"6134d58a-7ffc-43a4-aff6-4be8d977efc7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6134d58a-7ffc-43a4-aff6-4be8d977efc7\", [{\"type\": \"scatter\", \"y\": [2.1343105067296686, 1.1732566787564553, 34.35910918053895, 36.83795516371235, 2.808965074479856, 2.121072476666392, 14.710268306562307, 2.614184386432259, 3.7401716656949393, 3.731691310543067, 7.627658852038035, 22.752428302242212], \"x\": [-15.93675813378541, -29.152979217238133, 36.18954862666253, 37.49218733199513, -48.058829452570066, -8.941457938049755, 15.307792889226079, -34.70626581132249, 1.3891543686358903, -44.38375985168692, 7.013502082404112, 22.762748919711303], \"mode\": \"markers\", \"name\": \"Data\"}, {\"type\": \"scatter\", \"y\": [-4.587135767674729, -3.945015623654636, -3.3028954796345404, -2.6607753356144475, -2.0186551915943514, -1.3765350475742584, -0.7344149035541653, -0.09229475953406954, 0.5498253844860236, 1.1919455285061193, 1.8340656725262126, 2.476185816546307, 3.1183059605664014, 3.7604261045864957, 4.402546248606589, 5.0446663926266835, 5.686786536646778, 6.328906680666872, 6.971026824686967, 7.613146968707062, 8.255267112727154, 8.89738725674725, 9.539507400767343, 10.181627544787437, 10.823747688807533, 11.465867832827625, 12.10798797684772, 12.750108120867814, 13.392228264887907, 14.034348408908004, 14.676468552928096, 15.318588696948192, 15.960708840968286, 16.602828984988378, 17.244949129008475, 17.887069273028565, 18.529189417048663, 19.17130956106876, 19.81342970508885, 20.455549849108944, 21.09766999312904, 21.739790137149132, 22.38191028116923, 23.024030425189324, 23.666150569209417, 24.308270713229508, 24.9503908572496, 25.5925110012697, 26.234631145289793, 26.876751289309883], \"x\": [-48.058829452570066, -46.31289033451772, -44.56695121646536, -42.821012098413014, -41.07507298036066, -39.32913386230831, -37.58319474425596, -35.83725562620361, -34.09131650815126, -32.3453773900989, -30.599438272046555, -28.853499153994203, -27.10756003594185, -25.3616209178895, -23.61568179983715, -21.8697426817848, -20.123803563732448, -18.377864445680096, -16.631925327627744, -14.885986209575393, -13.140047091523044, -11.394107973470689, -9.64816885541834, -7.902229737365992, -6.156290619313637, -4.410351501261289, -2.6644123832089335, -0.9184732651565852, 0.827465852895763, 2.5734049709481184, 4.319344089000467, 6.065283207052822, 7.81122232510517, 9.557161443157518, 11.303100561209874, 13.049039679262222, 14.794978797314577, 16.540917915366933, 18.28685703341928, 20.03279615147163, 21.778735269523978, 23.524674387576326, 25.27061350562869, 27.016552623681036, 28.762491741733385, 30.508430859785733, 32.25436997783808, 34.000309095890444, 35.74624821394279, 37.49218733199513], \"mode\": \"line\", \"name\": \"Regression Line\"}], {\"title\": \"Linear Fit\", \"xaxis\": {\"title\": \"Change in water level (x)\"}, \"yaxis\": {\"title\": \"Water flowing out of the dam (y)\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_predict = np.linspace(X.min(),X.max())\n",
    "X_pred_bias = add_bias_term(X_predict)\n",
    "y_predict = X_pred_bias@theta_optimized\n",
    "\n",
    "trgdat = go.Scatter(x=X, y=y, mode=\"markers\", name = \"Data\")\n",
    "line = go.Scatter(x=X_predict, y=y_predict, mode=\"line\", name=\"Regression Line\")\n",
    "plotly.offline.iplot({\"data\": [trgdat,line],\n",
    "                      \"layout\": go.Layout(title = \"Linear Fit\",\n",
    "                                          xaxis = dict(title=\"Change in water level (x)\"),\n",
    "                                          yaxis = dict(title=\"Water flowing out of the dam (y)\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance\n",
    "An important concept in machine learning is the bias-variance tradeoff. Models with high bias are not complex enough for the data and tend to underfit, while models with high variance overfit to the training data. Aside from the course materials, an easy but informative read on this concept can be found [here](https://ml.berkeley.edu/blog/2017/07/13/tutorial-4/?imm_mid=0f493b&cmp=em-data-na-na-newsltr_ai_20170724).\n",
    "\n",
    "## Learning Curves\n",
    "\n",
    "We now implement code to generate the learning curves that will be useful in debugging learning algorithms. Recall that a learning curve plots training and cross validation error as a function of training set size.\n",
    "\n",
    "#### Implementation notes for `learning_curve()` function\n",
    "\n",
    "TODO: explain implementation note that I index out the ith values from the `_y` array (from training data) first and also wrap them up with np.asarray() because I need to use them in learning the parameters theta, meaning it will be used in cost and gradient functions.\n",
    "\n",
    "This also forced me to convert my above cost and gradient to not using len(y) anymore, but instead use y.size, since for the first iteration, `np.asarray(_y[i])` would give me a scalar in an array at least still letting it be one element size array, there will be a size attribute which can return the size as 1 \n",
    "\n",
    "In computing the cross val errors, we use the entire training dataset, why? see diary entry on 28 July Friday\n",
    "\n",
    "Function requires the bias term to be added to inputs prior to passing into the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve(_X, _y, _Xval, _yval, _lda):\n",
    "    # Initialize variables for computation inputs and storage\n",
    "    _trainerr = np.zeros_like(_y)\n",
    "    _cverr = np.zeros_like(_y)\n",
    "    numparams = _X.shape[1] # get the number of columns in our design matrix be equal to number of parameter needed to learn\n",
    "    # Select out subset of our training data, starting from just first data point to eventuall all the data points\n",
    "    for i in range(0, _y.size):\n",
    "        _yi = np.asarray(_y[:i+1]) \n",
    "        # Learn the parameters on the given subset of our training data, initialized to 0 prior to training      \n",
    "        _thetalearnt = fmin_cg(linear_regression_cost, np.zeros(numparams),\n",
    "                               linear_regression_gradient, (_X[:i+1], _yi, _lda),  \n",
    "                               disp=False)\n",
    "        # Compute the errors on the training and cross validation set\n",
    "        # We never regularize these errors, and so we hardcode lambda as 0\n",
    "        _trainerr[i] = linear_regression_cost(_thetalearnt,_X[:i+1], _yi, 0)\n",
    "        _cverr[i] = linear_regression_cost(_thetalearnt, _Xval, _yval, 0)\n",
    "    return _trainerr, _cverr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that both the train error and cross validation error are high when the number of training examples is increased. This reflects a high bias problem in the model – the linear regression model is too simple and is unable to fit our dataset well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"1f3b6c97-f76b-4015-aeef-8e3e95f3b88e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1f3b6c97-f76b-4015-aeef-8e3e95f3b88e\", [{\"type\": \"scatter\", \"y\": [3.944304526105059e-31, 2.8596207814261678e-30, 3.286595045501875, 2.8426776893998005, 13.154048809153679, 19.443962512506573, 20.09852165509372, 18.17285869520002, 22.609405424954723, 23.26146159261182, 24.31724958804416, 22.373906495108912], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Train\"}, {\"type\": \"scatter\", \"y\": [205.1210957454735, 110.30036610764684, 45.01023181030908, 48.368911369819756, 35.86514112316287, 33.829956818889514, 31.970986784706167, 30.862446323779825, 31.135997955024845, 28.936207468250167, 29.551431621997953, 29.433818129942686], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Cross Validation\"}], {\"title\": \"Learning curve for linear regression\", \"xaxis\": {\"title\": \"Number of training examples used to learn the parameters of our model\"}, \"yaxis\": {\"title\": \"Error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainerr, cverr = learning_curve(add_bias_term(X),y,add_bias_term(Xval),yval,0)\n",
    "\n",
    "numtrngegs = list(range(1,13))\n",
    "trainlearncurve = go.Scatter(x=numtrngegs, y=trainerr, mode=\"line\", name = \"Train\")\n",
    "cvlearncurve = go.Scatter(x=numtrngegs, y=cverr, mode=\"line\", name=\"Cross Validation\")\n",
    "plotly.offline.iplot({\"data\": [trainlearncurve,cvlearncurve],\n",
    "                      \"layout\": go.Layout(title = \"Learning curve for linear regression\",\n",
    "                                          xaxis = dict(title=\"Number of training examples used to learn the parameters of our model\"),\n",
    "                                          yaxis = dict(title=\"Error\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "We now overcome the initial problem of underfitting by a linear model, by adding more features, and hence more parameters to learn, with a polynomial model. We will add more features by using the higher powers of the one and only existing feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poly_features(_X, degree):\n",
    "    polyfeats = map(lambda pwr : _X**pwr, range(1,degree+1)) # iterable object of power raised arrays\n",
    "    _Xpoly = np.vstack(polyfeats).T  #Perform a transpose to get the matrix into our desired dimension m by p\n",
    "    return _Xpoly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation notes for `feature_normalize()` function\n",
    "\n",
    "The MATLAB skeleton code implementing feature normalization as given in this course, is similar to an example in the [MATLAB documentation for `bsxfun()`](https://www.mathworks.com/help/matlab/ref/bsxfun.html), and looks like:\n",
    "```\n",
    "mu = mean(X);\n",
    "X_norm = bsxfun(@minus, X, mu);\n",
    "sigma = std(X_norm);\n",
    "X_norm = bsxfun(@rdivide, X_norm, sigma);\n",
    "```\n",
    "All the MATLAB functions were used in their default configuration. In order for our python implementation to be consistent, we have to do the following:\n",
    "\n",
    "- Mean and standard deviation for MATLAB matrices (2D arrays) are computed column-wise returning vectors by default, whereas in Numpy, they are computed globally for all elements in the flattened array, returning scalars. Passing in `axis=0` ensures that we replicate the MATLAB behavior in Numpy.\n",
    "\n",
    "- In MATLAB, standard deviation is normalized by $N-1$ by default (i.e. degrees of freedom is 1), where $N$ is the number of observations. In Numpy, standard deviation is normalized by $N$ by default (i.e. degrees of freedom is 0), so passing in `ddof=1` ensures that we match the default MATLAB behavior.\n",
    "\n",
    "Broadcasting happens automatically in Numpy based on the dimensions of the arrays, without the need to call a function, unlike in MATLAB where it is necessary to call `bsxfun()`.\n",
    "\n",
    "TODO: This also reminds me that in programming exercise 1, where we need to do feature normalization, I also have a big issue to fix..... just calling np.mean() on an array does not mean its a vectorized mean, but a global mean of all the elements in the array...so I gotta go back to fix it....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_normalize(_X):\n",
    "    _mu = np.mean(_X, axis=0)\n",
    "    _Xnorm = _X-_mu\n",
    "    _sigma = np.std(_Xnorm, axis=0, ddof=1)\n",
    "    _Xnorm = _Xnorm / _sigma\n",
    "    return _Xnorm, _mu, _sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Polynomial Regression\n",
    "\n",
    "We will train a polynomial of degree 8. Feature normalization is necessary because high polynomial powers of the the higher order features that we are generating give rise to large values that would make training problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 8\n",
    "Xpoly,mu,sigma = feature_normalize(poly_features(X,deg))\n",
    "#Qn why do we use the mu and sigma from the training data to normalize the cross validation and test datasets?\n",
    "# Compute feature scaling for cross validation data\n",
    "Xvalpoly =poly_features(Xval,deg)\n",
    "Xvalpoly = Xvalpoly-mu\n",
    "Xvalpoly = Xvalpoly / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.145323\n",
      "         Iterations: 66\n",
      "         Function evaluations: 142\n",
      "         Gradient evaluations: 130\n"
     ]
    }
   ],
   "source": [
    "lda_init = 0\n",
    "theta_optimized = train_linear_regression(add_bias_term(Xpoly), y, lda_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize our model with the parameters we have learnt, we need to also normalize the features of our artificial data set that \"spans\" the entire data space, similar to in Programming Exercise 1. The predictions of the model using this \"spanning\" dataset as input would be directly map to the original output scale and do not require any normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"ba798f5c-41d3-46fd-91f7-6aa09ca38ab5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ba798f5c-41d3-46fd-91f7-6aa09ca38ab5\", [{\"type\": \"scatter\", \"y\": [2.1343105067296686, 1.1732566787564553, 34.35910918053895, 36.83795516371235, 2.808965074479856, 2.121072476666392, 14.710268306562307, 2.614184386432259, 3.7401716656949393, 3.731691310543067, 7.627658852038035, 22.752428302242212], \"x\": [-15.93675813378541, -29.152979217238133, 36.18954862666253, 37.49218733199513, -48.058829452570066, -8.941457938049755, 15.307792889226079, -34.70626581132249, 1.3891543686358903, -44.38375985168692, 7.013502082404112, 22.762748919711303], \"mode\": \"markers\", \"name\": \"Data\"}, {\"type\": \"scatter\", \"y\": [-24.657527692481608, -12.994508542601226, -5.396186645610605, -0.7194741946014216, 1.9227288134335476, 3.2055334568490714, 3.630610068016557, 3.559724928613694, 3.2436870186195677, 2.8471086759832485, 2.4693663438173608, 2.162129901851998, 1.9438113987679648, 1.8112663219118352, 1.7490628607792054, 1.7366169405358363, 1.753473121730274, 1.7829937822350455, 1.8147013173372104, 1.8455004137827153, 1.8799897734624929, 1.9300549823121287, 2.0139165398802716, 2.154789384904895, 2.379292572119884, 2.715730075398221, 3.192346012221719, 3.8356399043506397, 4.668809909450571, 5.7103742783171265, 6.973003612223049, 8.46257881479571, 10.177471952716697, 12.108029559418883, 14.236220235839864, 16.53539072217445, 18.97005693445251, 21.4956387796518, 24.058029882939614, 26.592875680520358, 29.024415651449562, 31.263727781659153, 33.206195673321886, 34.72800103256652, 35.68142658843945, 35.888736815891285, 35.13438615545155, 33.15528674213726, 29.628849976025688, 24.158498586804644], \"x\": [-58.058829452570066, -55.90472706921159, -53.75062468585312, -51.59652230249465, -49.44241991913617, -47.28831753577769, -45.13421515241922, -42.98011276906075, -40.826010385702276, -38.6719080023438, -36.51780561898533, -34.36370323562686, -32.20960085226838, -30.055498468909907, -27.901396085551433, -25.747293702192962, -23.593191318834485, -21.439088935476008, -19.284986552117537, -17.130884168759067, -14.97678178540059, -12.822679402042112, -10.668577018683642, -8.514474635325172, -6.3603722519666945, -4.206269868608217, -2.052167485249747, 0.10193489810872336, 2.2560372814672007, 4.410139664825678, 6.564242048184141, 8.718344431542619, 10.872446814901096, 13.026549198259573, 15.18065158161805, 17.334753964976514, 19.48885634833499, 21.64295873169347, 23.797061115051932, 25.95116349841041, 28.105265881768887, 30.259368265127364, 32.41347064848584, 34.567573031844304, 36.72167541520278, 38.87577779856126, 41.02988018191972, 43.1839825652782, 45.33808494863668, 47.49218733199513], \"mode\": \"line\", \"name\": \"Regression Curve\"}], {\"title\": \"Polynomial Fit\", \"xaxis\": {\"title\": \"Change in water level (x)\"}, \"yaxis\": {\"title\": \"Water flowing out of the dam (y)\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "span = np.linspace(X.min()-10,X.max()+10)  # Add additional spaces at the ends to demonstrate the over fitting\n",
    "# spanpoly is just a defined range of X values to scale for plotting a curve\n",
    "# Normalize the features in spanpoly before making predictions\n",
    "spanpoly = poly_features(span,deg)\n",
    "spanpoly = spanpoly-mu\n",
    "spanpoly = spanpoly / sigma\n",
    "yspan = add_bias_term(spanpoly)@theta_optimized\n",
    "#print(ypoly_predict)\n",
    "trgdat = go.Scatter(x=X, y=y, mode=\"markers\", name = \"Data\")\n",
    "line = go.Scatter(x=span, y=yspan, mode=\"line\", name=\"Regression Curve\")\n",
    "plotly.offline.iplot({\"data\": [trgdat,line],\n",
    "                      \"layout\": go.Layout(title = \"Polynomial Fit\",\n",
    "                                          xaxis = dict(title=\"Change in water level (x)\"),\n",
    "                                          yaxis = dict(title=\"Water flowing out of the dam (y)\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the polynomial fit is able to follow the datapoints very well - thus, obtaining a low training error. However, the polynomial fit is very complex and even drops off at the extremes. This is an indicator that the polynomial regression model is overfitting the training data and will not generalize well.\n",
    "\n",
    "This is further verified by the learning curves below where the training error is low, but the cross validation error is high. There is a gap between the training and cross validation errors, indicating a problem of high variance.\n",
    "\n",
    "TODO: The trends match those of figure 5 in ex5.pdf, but the actual values of the points on these curves do not. Find out why..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"d86df1dd-fad3-4bda-acff-fc275600a3b0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d86df1dd-fad3-4bda-acff-fc275600a3b0\", [{\"type\": \"scatter\", \"y\": [0.0, 3.929513384132165e-29, 1.9197822034516016e-11, 1.035327866232389e-21, 3.1946014919700054e-09, 1.9593073920726024e-08, 2.891238185717503e-10, 2.1086274821551446e-09, 0.0012067647709411574, 0.010819132178404649, 0.039119581262833776, 0.1453233080324529], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Train\"}, {\"type\": \"scatter\", \"y\": [160.72189969184072, 160.12151033409683, 61.75500548018491, 61.928895405412504, 6.596559401246538, 10.640486560955924, 27.988093109621374, 22.700261100621287, 31.22681539268776, 96.23896478143672, 24.578652298940142, 23.751563342098198], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Cross Validation\"}], {\"title\": \"Learning curve for polynomial regression\", \"xaxis\": {\"title\": \"Number of training examples used to learn the parameters of our model\"}, \"yaxis\": {\"title\": \"Error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "polytrainerr, polycverr = learning_curve(add_bias_term(Xpoly),y,add_bias_term(Xvalpoly),yval,lda_init)\n",
    "\n",
    "numtrngegs = list(range(1,13))\n",
    "polytrainlearncurve = go.Scatter(x=numtrngegs, y=polytrainerr, mode=\"line\", name = \"Train\")\n",
    "polycvlearncurve = go.Scatter(x=numtrngegs, y=polycverr, mode=\"line\", name=\"Cross Validation\")\n",
    "plotly.offline.iplot({\"data\": [polytrainlearncurve,polycvlearncurve],\n",
    "                      \"layout\": go.Layout(title = \"Learning curve for polynomial regression\",\n",
    "                                          xaxis = dict(title=\"Number of training examples used to learn the parameters of our model\"),\n",
    "                                          yaxis = dict(title=\"Error\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the Regularization Parameter $\\lambda$\n",
    "\n",
    "Let us explore how different values for the regularization parameter affects the quality of our polynomial fittings, and their corresponding learning curves. The results speak for themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For $\\lambda = 1$, the model is good. \n",
    "\n",
    "It does not suffer from high bias or high variance, effectively achieving a good trade-off between bias and variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 12.488335\n",
      "         Iterations: 7\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"0dfe657a-bf2c-43b4-a6ef-b168876664ff\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0dfe657a-bf2c-43b4-a6ef-b168876664ff\", [{\"type\": \"scatter\", \"y\": [2.1343105067296686, 1.1732566787564553, 34.35910918053895, 36.83795516371235, 2.808965074479856, 2.121072476666392, 14.710268306562307, 2.614184386432259, 3.7401716656949393, 3.731691310543067, 7.627658852038035, 22.752428302242212], \"x\": [-15.93675813378541, -29.152979217238133, 36.18954862666253, 37.49218733199513, -48.058829452570066, -8.941457938049755, 15.307792889226079, -34.70626581132249, 1.3891543686358903, -44.38375985168692, 7.013502082404112, 22.762748919711303], \"mode\": \"markers\", \"name\": \"Data\"}, {\"type\": \"scatter\", \"y\": [9.046902819914934, 7.2306601205277685, 5.834604220354874, 4.770407475804763, 3.967009140962375, 3.3677754726063465, 2.9280331015372667, 2.612940951132297, 2.395667752130896, 2.255843970745426, 2.1782587352795444, 2.1517741145265266, 2.168430869308719, 2.222721566608487, 2.311008713830202, 2.4310673388219044, 2.581733209374462, 2.7626396540051545, 2.9740277139217803, 3.2166161241525466, 3.4915193899160717, 3.8002039923950415, 4.144474526166208, 4.526483338628507, 4.948759009860253, 5.414250779425497, 5.92638779473881, 6.489153823686864, 7.107179842294321, 7.785858676310683, 8.53148764368398, 9.3514469129762, 10.254423060864518, 11.250689079961692, 12.352453856277867, 13.574295903735413, 14.933697911237473, 16.451700425879896, 18.15369476398573, 20.07037700973023, 22.23888672921359, 24.704155795928, 27.52049449165429, 30.75344381491299, 34.48192469718374, 38.80071659519564, 43.823299695681044, 49.68509673707389, 56.547152220722815, 64.60028855227878], \"x\": [-58.058829452570066, -55.90472706921159, -53.75062468585312, -51.59652230249465, -49.44241991913617, -47.28831753577769, -45.13421515241922, -42.98011276906075, -40.826010385702276, -38.6719080023438, -36.51780561898533, -34.36370323562686, -32.20960085226838, -30.055498468909907, -27.901396085551433, -25.747293702192962, -23.593191318834485, -21.439088935476008, -19.284986552117537, -17.130884168759067, -14.97678178540059, -12.822679402042112, -10.668577018683642, -8.514474635325172, -6.3603722519666945, -4.206269868608217, -2.052167485249747, 0.10193489810872336, 2.2560372814672007, 4.410139664825678, 6.564242048184141, 8.718344431542619, 10.872446814901096, 13.026549198259573, 15.18065158161805, 17.334753964976514, 19.48885634833499, 21.64295873169347, 23.797061115051932, 25.95116349841041, 28.105265881768887, 30.259368265127364, 32.41347064848584, 34.567573031844304, 36.72167541520278, 38.87577779856126, 41.02988018191972, 43.1839825652782, 45.33808494863668, 47.49218733199513], \"mode\": \"line\", \"name\": \"Regression Curve\"}], {\"title\": \"Polynomial fit when lambda = 1 (just right)\", \"xaxis\": {\"title\": \"Change in water level (x)\"}, \"yaxis\": {\"title\": \"Water flowing out of the dam (y)\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda_init = 1\n",
    "theta_optimized = train_linear_regression(add_bias_term(Xpoly), y, lda_init)\n",
    "yspan = add_bias_term(spanpoly)@theta_optimized\n",
    "trgdat = go.Scatter(x=X, y=y, mode=\"markers\", name = \"Data\")\n",
    "line = go.Scatter(x=span, y=yspan, mode=\"line\", name=\"Regression Curve\")\n",
    "plotly.offline.iplot({\"data\": [trgdat,line],\n",
    "                      \"layout\": go.Layout(title = \"Polynomial fit when lambda = 1 (just right)\",\n",
    "                                          xaxis = dict(title=\"Change in water level (x)\"),\n",
    "                                          yaxis = dict(title=\"Water flowing out of the dam (y)\"))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"ecefec15-0274-471d-932c-1aee1ad01ecc\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ecefec15-0274-471d-932c-1aee1ad01ecc\", [{\"type\": \"scatter\", \"y\": [0.06822023516495304, 0.051339935729074186, 3.8661481786601737, 1.4749762268029336, 1.647946654439197, 1.2658908611088802, 2.8305854256473393, 2.753338295392951, 1.9509761827172676, 2.3143364238187636, 1.4747785497748997, 2.3709719329829477], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Train\"}, {\"type\": \"scatter\", \"y\": [160.94276613110603, 161.6050857775477, 91.16016621536417, 130.6220545120399, 14.453376871506775, 13.835965685857019, 7.465511973103134, 6.995836014492041, 9.54043987952234, 7.319787329997435, 7.193409395111996, 5.116095527107636], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Cross Validation\"}], {\"title\": \"Learning curve for polynomial regression when lambda = 1 (just right)\", \"xaxis\": {\"title\": \"Number of training examples used to learn the parameters of our model\"}, \"yaxis\": {\"title\": \"Error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "polytrainerr, polycverr = learning_curve(add_bias_term(Xpoly),y,add_bias_term(Xvalpoly),yval,lda_init)\n",
    "numtrngegs = list(range(1,13))\n",
    "polytrainlearncurve = go.Scatter(x=numtrngegs, y=polytrainerr, mode=\"line\", name = \"Train\")\n",
    "polycvlearncurve = go.Scatter(x=numtrngegs, y=polycverr, mode=\"line\", name=\"Cross Validation\")\n",
    "plotly.offline.iplot({\"data\": [polytrainlearncurve,polycvlearncurve],\n",
    "                      \"layout\": go.Layout(title = \"Learning curve for polynomial regression when lambda = 1 (just right)\",\n",
    "                                          xaxis = dict(title=\"Number of training examples used to learn the parameters of our model\"),\n",
    "                                          yaxis = dict(title=\"Error\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For $\\lambda = 100$, the model is bad, underfitting results. \n",
    "\n",
    "It fits the training data poorly and performs poorly on the cross-validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 123.787166\n",
      "         Iterations: 1\n",
      "         Function evaluations: 50\n",
      "         Gradient evaluations: 40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"2b2721f2-1e8a-419f-a926-f6a20977b8ad\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"2b2721f2-1e8a-419f-a926-f6a20977b8ad\", [{\"type\": \"scatter\", \"y\": [2.1343105067296686, 1.1732566787564553, 34.35910918053895, 36.83795516371235, 2.808965074479856, 2.121072476666392, 14.710268306562307, 2.614184386432259, 3.7401716656949393, 3.731691310543067, 7.627658852038035, 22.752428302242212], \"x\": [-15.93675813378541, -29.152979217238133, 36.18954862666253, 37.49218733199513, -48.058829452570066, -8.941457938049755, 15.307792889226079, -34.70626581132249, 1.3891543686358903, -44.38375985168692, 7.013502082404112, 22.762748919711303], \"mode\": \"markers\", \"name\": \"Data\"}, {\"type\": \"scatter\", \"y\": [-14.528701305176787, -11.547165586416995, -9.106589667023039, -7.116223700524364, -5.498733271620336, -4.188533935474941, -3.130286829564742, -2.277544687335822, -1.5915379340410845, -1.0400908962448656, -0.5966585075974818, -0.2394742445980114, 0.04919962282079182, 0.283728029935901, 0.4756735780405509, 0.6342893192330534, 0.766941375891436, 0.8794685544217747, 0.9764857617524786, 1.0616376819311197, 1.1378088190647746, 1.2072956617291966, 1.271946372856503, 1.3332730579954177, 1.3925413137224625, 1.450841406866858, 1.5091450840962608, 1.5683516602947971, 1.6293266830492388, 1.6929361194435213, 1.7600786602461416, 1.83171838545937, 1.9089196840835292, 1.9928859698339856, 2.0850033834328383, 2.186890320981657, 2.3004532768059756, 2.4279491380466163, 2.572053717157261, 2.7359359573520723, 2.923336894931492, 3.1386521112987364, 3.387016056363847, 3.674386273916517, 4.007625208433294, 4.394576921669058, 4.844135696267137, 5.366303152506667, 5.972230153190243, 6.674239420559217], \"x\": [-58.058829452570066, -55.90472706921159, -53.75062468585312, -51.59652230249465, -49.44241991913617, -47.28831753577769, -45.13421515241922, -42.98011276906075, -40.826010385702276, -38.6719080023438, -36.51780561898533, -34.36370323562686, -32.20960085226838, -30.055498468909907, -27.901396085551433, -25.747293702192962, -23.593191318834485, -21.439088935476008, -19.284986552117537, -17.130884168759067, -14.97678178540059, -12.822679402042112, -10.668577018683642, -8.514474635325172, -6.3603722519666945, -4.206269868608217, -2.052167485249747, 0.10193489810872336, 2.2560372814672007, 4.410139664825678, 6.564242048184141, 8.718344431542619, 10.872446814901096, 13.026549198259573, 15.18065158161805, 17.334753964976514, 19.48885634833499, 21.64295873169347, 23.797061115051932, 25.95116349841041, 28.105265881768887, 30.259368265127364, 32.41347064848584, 34.567573031844304, 36.72167541520278, 38.87577779856126, 41.02988018191972, 43.1839825652782, 45.33808494863668, 47.49218733199513], \"mode\": \"line\", \"name\": \"Regression Curve\"}], {\"title\": \"Polynomial fit when lambda=100 (underfitting)\", \"xaxis\": {\"title\": \"Change in water level (x)\"}, \"yaxis\": {\"title\": \"Water flowing out of the dam (y)\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda_init = 100\n",
    "theta_optimized = train_linear_regression(add_bias_term(Xpoly), y, lda_init)\n",
    "yspan = add_bias_term(spanpoly)@theta_optimized\n",
    "trgdat = go.Scatter(x=X, y=y, mode=\"markers\", name = \"Data\")\n",
    "line = go.Scatter(x=span, y=yspan, mode=\"line\", name=\"Regression Curve\")\n",
    "plotly.offline.iplot({\"data\": [trgdat,line],\n",
    "                      \"layout\": go.Layout(title = \"Polynomial fit when lambda=100 (underfitting)\",\n",
    "                                          xaxis = dict(title=\"Change in water level (x)\"),\n",
    "                                          yaxis = dict(title=\"Water flowing out of the dam (y)\"))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"ecc935d3-b837-46ed-a1d3-1d3918ab90db\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"ecc935d3-b837-46ed-a1d3-1d3918ab90db\", [{\"type\": \"scatter\", \"y\": [2.0840858369352078, 1.2792420745634152, 169.30212472078523, 248.29856393156342, 205.0338682353101, 171.21507109022377, 158.82963413008568, 139.6509207257672, 124.58843196107878, 114.08877831259434, 105.62828858386311, 113.03266444407973], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Train\"}, {\"type\": \"scatter\", \"y\": [163.86198723532405, 163.81769847977804, 141.88651320511272, 130.47396725382634, 131.40398486349196, 131.44264911794113, 131.1625230298952, 131.20498601917316, 131.3842497906319, 131.3800143849846, 131.65607795125345, 131.0168393542986], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Cross Validation\"}], {\"title\": \"Learning curve for polynomial regression when lambda = 100 (underfitting)\", \"xaxis\": {\"title\": \"Number of training examples used to learn the parameters of our model\"}, \"yaxis\": {\"title\": \"Error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "polytrainerr, polycverr = learning_curve(add_bias_term(Xpoly),y,add_bias_term(Xvalpoly),yval,lda_init)\n",
    "numtrngegs = list(range(1,13))\n",
    "polytrainlearncurve = go.Scatter(x=numtrngegs, y=polytrainerr, mode=\"line\", name = \"Train\")\n",
    "polycvlearncurve = go.Scatter(x=numtrngegs, y=polycverr, mode=\"line\", name=\"Cross Validation\")\n",
    "plotly.offline.iplot({\"data\": [polytrainlearncurve,polycvlearncurve],\n",
    "                      \"layout\": go.Layout(title = \"Learning curve for polynomial regression when lambda = 100 (underfitting)\",\n",
    "                                          xaxis = dict(title=\"Number of training examples used to learn the parameters of our model\"),\n",
    "                                          yaxis = dict(title=\"Error\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting $\\lambda$ using a cross validation set\n",
    "\n",
    "As different values of $\\lambda$ can also affect cross validation set performance, and hence the generalizability of our model, we also want to systematically search for optimal values of $\\lambda$. In the machine learning community, $\\lambda$ is called a **hyperparameter**, as its specific chosen value can constrain or determine what values we learn for our parameters $\\theta_1 \\dots \\theta_n$ during the training procedure.\n",
    "\n",
    "The curves that we draw to compare training and cross-validation errors for varying values of $\\lambda$ are called **validation curves**. The function `validation_curve()` is similar to `learning_curve()`. \n",
    "\n",
    "TODO: clarify why I also do not regularize the calculation of the errors here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_curve(_X, _y, _Xval, _yval, _ldas):\n",
    "    _trainerr = np.zeros_like(_ldas)\n",
    "    _cverr  = np.zeros_like(_ldas)\n",
    "    _thetainit = np.zeros(_X.shape[1])\n",
    "    for i in range(0,len(_ldas)):\n",
    "        _thetalearnt = fmin_cg(linear_regression_cost, _thetainit,\n",
    "                               linear_regression_gradient, (_X, _y, _ldas[i]), disp=False)\n",
    "        _trainerr[i] = linear_regression_cost(_thetalearnt, _X, _y, 0) #do not regularize the errors?\n",
    "        _cverr[i] = linear_regression_cost(_thetalearnt, _Xval, _yval, 0)\n",
    "    return _trainerr, _cverr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"0c50d68b-65f1-4f01-930d-6b6a24be034b\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"0c50d68b-65f1-4f01-930d-6b6a24be034b\", [{\"type\": \"scatter\", \"y\": [0.1453233080324529, 0.11490385425284966, 0.17245793410831595, 0.22266721284341578, 0.30369042852338446, 0.5733584551349399, 0.9172549381023534, 2.3709719329829477, 5.86684225495429, 20.6946553625258], \"x\": [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10], \"mode\": \"line\", \"name\": \"Train\"}, {\"type\": \"scatter\", \"y\": [23.751563342098198, 10.238874880554562, 17.56416208344051, 17.304494390521633, 11.822468941111536, 6.180067203476629, 4.643764049362694, 5.116095527107636, 4.076583573142856, 16.782576023269357], \"x\": [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10], \"mode\": \"line\", \"name\": \"Cross Validation\"}], {\"title\": \"Validation curves for polynomial regression to select lambda\", \"xaxis\": {\"title\": \"lambda\"}, \"yaxis\": {\"title\": \"Error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lambdas = [0,0.001,0.003,0.01,0.03,0.1,0.3,1,1.5,2,2.5,3,3.5,4,5,6,7,8,9,10]\n",
    "lambdas = [0,0.001,0.003,0.01,0.03,0.1,0.3,1,3,10]\n",
    "valcurvetrgerr, valcurvecverr = validation_curve(add_bias_term(Xpoly),y,add_bias_term(Xvalpoly),yval,lambdas)\n",
    "\n",
    "polytrainlearncurve = go.Scatter(x=lambdas, y=valcurvetrgerr, mode=\"line\", name = \"Train\")\n",
    "polycvlearncurve = go.Scatter(x=lambdas, y=valcurvecverr, mode=\"line\", name=\"Cross Validation\")\n",
    "plotly.offline.iplot({\"data\": [polytrainlearncurve,polycvlearncurve],\n",
    "                      \"layout\": go.Layout(title = \"Validation curves for polynomial regression to select lambda\",\n",
    "                                          xaxis = dict(title=\"lambda\"),\n",
    "                                          yaxis = dict(title=\"Error\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observ that the best value of $\\lambda$ is at $3$, because that is the value that **minimizes the cross-validation error**.\n",
    "\n",
    "TODO: The trends match those of figure 9 in ex5.pdf, but the actual values of the points on these curves do not. Find out why...\n",
    "\n",
    "TODO: Need to clarify what happens when I insert more lambda values in between... from 5,6,7,8,9.... I would see more details and thus a somewhat different curve....would that affect my decision on 3 though?? For example using `lambdas =[0,0.001,0.003,0.01,0.03,0.1,0.3,1,1.5,2,2.5,3,3.5,4,5,6,7,8,9,10]`, I see that the best result is actually at lambda=2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Test Set Error\n",
    "\n",
    "To get a better indication of the model’s performance in the real world, it is important to evaluate the “final” model on a test set that was not used in any part of training (that is, it was neither used to select the λ parameters, nor to learn the model parameters θ).\n",
    "\n",
    "TODO: Figure out why even though I followed all the steps here should be correct, I got such a different test error as the assignment PDF... unexpected value of 4.33 when it should've ben 3.8599..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 31.721688\n",
      "         Iterations: 5\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 11.22065879,   6.10725898,   3.57159262,   3.86964078,\n",
       "         2.29166079,   2.37879646,   1.44963878,   1.43976211,   0.90654477])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_init = 3\n",
    "theta_optimized = train_linear_regression(add_bias_term(Xpoly), y, lda_init)\n",
    "theta_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3397328988063162"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute feature scaling for test data, using the mu and sigma obtained from computing on the test data\n",
    "Xtestpoly = poly_features(Xtest,deg)\n",
    "Xtestpoly = Xtestpoly-mu\n",
    "Xtestpoly = Xtestpoly / sigma\n",
    "linear_regression_cost(theta_optimized, add_bias_term(Xtestpoly), ytest, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting learning curves with randomly selected examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explain the implementation here. The random selection of subsets of training examples was obtained through using a boolean mask and randomly permuting its rows. We then use this permuted boolean mask to select out rows randomly the array, thereby achieving random selection for a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numelm is the number of examples we want to select out with the mask, ie. number that controls\n",
    "# the size of our subset\n",
    "def generate_boolean_mask(_length, _numegs):\n",
    "    _mask = np.zeros(_length, dtype=bool)\n",
    "    for i in range(0, _numegs):\n",
    "        _mask[i] = True\n",
    "    return _mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: The number of rows (examples) of the training dataset is not equal to number of rows of the validation dataset, so do we still use the same mask `_length` for both training data and validation data, and index out an equal number of training examples from both? Must we also always index out the same random subset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve_with_rand_chosen_examples(_X, _y, _Xval, _yval, _lda, _numran):\n",
    "    # Initialize variables for computation inputs and storage\n",
    "    _trainerr = np.zeros_like(_y)\n",
    "    _cverr = np.zeros_like(_y)\n",
    "    numparams = _X.shape[1] # get the number of columns in our design matrix be equal to number of parameter needed to learn\n",
    "    # Select out subset of our training data, starting from just first data point to eventuall all the data points\n",
    "    for i in range(0, _y.size):\n",
    "        count = 0\n",
    "        errtrg = np.zeros(_numran) # accumulate the errors from random selection into a list\n",
    "        errval = np.zeros(_numran)\n",
    "        # prepare unshuffled boolean mask \n",
    "        mask = generate_boolean_mask(_X.shape[0], i+1)\n",
    "        while(count<50):\n",
    "            # Shuffle the mask at the start of each iteration, by randomly permuting the rows\n",
    "            np.random.shuffle(mask)\n",
    "            # masking out elements using the same randomly shuffled boolean mask has the effect of performing\n",
    "            # random selection of a subset of i training examples\n",
    "            _Xmasked = np.compress(mask, _X, axis=0)\n",
    "            _ymasked = np.compress(mask, _y)\n",
    "            _Xvalmasked = np.compress(mask, _Xval, axis=0)\n",
    "            _yvalmasked = np.compress(mask, _yval)\n",
    "            # Learn the parameters on the given subset of our training data, parameters initialized to 0 prior to training            \n",
    "            _thetalearnt = fmin_cg(linear_regression_cost, np.zeros(numparams),\n",
    "                                    linear_regression_gradient, (_Xmasked, _ymasked, _lda),  \n",
    "                                    disp=False)\n",
    "            # Compute the errors on the given subset of training and cross validation data\n",
    "            # we accumulate 50 of errors into an array, values to be averaged out and stored later\n",
    "            errtrg[count] = linear_regression_cost(_thetalearnt, _Xmasked, _ymasked, 0)\n",
    "            errval[count] = linear_regression_cost(_thetalearnt, _Xvalmasked, _yvalmasked, 0)\n",
    "            count = count+1\n",
    "            \n",
    "        _trainerr[i] = np.mean(errtrg)\n",
    "        _cverr[i] = np.mean(errval)\n",
    "        \n",
    "    return _trainerr, _cverr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"db573f60-75cb-40c3-865e-729952548f02\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"db573f60-75cb-40c3-865e-729952548f02\", [{\"type\": \"scatter\", \"y\": [0.013782850229650458, 0.017712084142885002, 0.015770214739318167, 0.2496751154997705, 0.176250540948429, 0.7180820978254058, 0.3604744640546473, 0.1982755991316648, 0.27272351153872937, 0.8034698541714824, 0.21913688712184345, 0.230953194966121], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Train\"}, {\"type\": \"scatter\", \"y\": [120.59771566563671, 139.47293542962566, 71.51704878186628, 42.321106312460735, 25.583356996689282, 20.28579587694178, 22.77016374596944, 11.522522115612405, 9.192876578869228, 7.817339520751052, 8.313476660320717, 8.616780997516603], \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"mode\": \"line\", \"name\": \"Cross Validation\"}], {\"title\": \"Polynomial regression learning curve with randomly selected examples, (lambda = 0.01)\", \"xaxis\": {\"title\": \"Number of training examples used to learn the parameters of our model\"}, \"yaxis\": {\"title\": \"Error\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda_init = 0.01\n",
    "numran = 50 # Number of times to perform the random selection for a given size of subset of training examples\n",
    "randtrainerr, randcverr = learning_curve_with_rand_chosen_examples(add_bias_term(Xpoly),y,\n",
    "                                                                   add_bias_term(Xvalpoly),yval,lda_init, numran)\n",
    "\n",
    "numtrngegs = list(range(1,13))\n",
    "randtrainlearncurve = go.Scatter(x=numtrngegs, y=randtrainerr, mode=\"line\", name = \"Train\")\n",
    "randcvlearncurve = go.Scatter(x=numtrngegs, y=randcverr, mode=\"line\", name=\"Cross Validation\")\n",
    "plotly.offline.iplot({\"data\": [randtrainlearncurve,randcvlearncurve],\n",
    "                      \"layout\": go.Layout(title = \"Polynomial regression learning curve with randomly selected examples, (lambda = 0.01)\",\n",
    "                                          xaxis = dict(title=\"Number of training examples used to learn the parameters of our model\"),\n",
    "                                          yaxis = dict(title=\"Error\"))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Trend is similar to figure 10 in ex5.pdf, but the actual values of the points on the curve are not, find out why..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testcases\n",
    "\n",
    "WARNING: Messy WIP below....\n",
    "\n",
    "Aside from the basic test values which we are told to compute in the assignment PDF itself, the course forums have test cases which are more rigorous. These testcases are executed here....\n",
    "\n",
    "TODO: Wrap up all the below test cases into a single python module which will be imported directly into this notebook and executed in the cells below this one..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function outputs match expected values.\n",
    "\n",
    "TODO: Start to use formal unit testing constructs like `assert()` but they may not work too, because my values are not exact with the expected Octave output from the course skeleton code...\n",
    "\n",
    "well this is a simple test case that was part of the assignment PDF, so I wonder if this should be wrapped in the module, or can it be a part of the work above (probably be a part of the work above...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304.03485888693092"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the cost function\n",
    "X_bias = add_bias_term(X)\n",
    "Theta = np.array([1,1])# Need to make Theta a column vector\n",
    "linear_regression_cost(Theta, X_bias,y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Think of collating all the rigorous test cases from the course, and wrapping them up into a \n",
    "### single module that instantiates the TestCase() instance and runs unit tests to assert the truth of floating point\n",
    "### outputs. Do not substitue these unit tests for places where I want to examine the outputs visually, as these are\n",
    "### parts of the exercise where the correct output is given to us visually for us to manually inspect and compare \n",
    "### with the outputs of our own functions....\n",
    "\n",
    "### See here below is an example...\n",
    "#from unittest.TestCase import assertAlmostEqual\n",
    "import unittest as utest\n",
    "test = utest.TestCase()\n",
    "\n",
    "test.assertAlmostEqual(first=linear_regression_cost(Theta, X_bias,y,1), second=303.993, delta = 0.5)\n",
    "\n",
    "### Once I have coded out the module, just import it into this notebook, and add a chunk at the end where\n",
    "### I subject all these functions to the unit tests just to show that they are correct. Purpose of this task is \n",
    "### to just get myself to pick up how to do unit testing.\n",
    "\n",
    "#### Added on 28 July 2017\n",
    "# Since coding out these unit tests are gonna take quite a lot of time, and I know that my functions here\n",
    "# already correct, and its simple cases here since we only have 12 training examples, maybe I won't waste time here\n",
    "# writing unit tests. But I want to still write these unit tests for practice so I will do it for Exercise 1, where \n",
    "# I've always suspected something was amiss with the regression on a plane.... may be wrong,so I can also implement\n",
    "# gradient checking there, plus change the code to stop using numpy matrices and just use ndarrays with @ operator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient function outputs match the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -15.30301567,  598.25074417])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_gradient(Theta,X_bias,y,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further testing cases\n",
    "TODO: wrap this up into unit test module that I've written about earilier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.4        -8.73333333 -4.33333333 -7.93333333]\n",
      "[-1.4        -8.26666667 -3.63333333 -7.        ]\n"
     ]
    }
   ],
   "source": [
    "Xtestcase = np.array([[1,8,1,6],[1, 3,5 ,7],[1,4,9,2]])\n",
    "ytestcase = np.array([7,6,5])\n",
    "thetatest = np.array([0.1,0.2,0.3,0.4])\n",
    "print(linear_regression_gradient(thetatest,Xtestcase,ytestcase,0)) # lamdba = 0\n",
    "print(linear_regression_gradient(thetatest,Xtestcase,ytestcase,7)) # lamdba = 7\n",
    "#test case results match the expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.05 but expected 3.015\n",
      "[-2.  -2.6 -3.9 -5.2] as expected\n"
     ]
    }
   ],
   "source": [
    "##More Test cases\n",
    "xtc = np.array([1,2,3,4]); ytc=np.array([5]); thetatc = np.array([0.1,0.2,0.3,0.4])\n",
    "print(linear_regression_cost(thetatc,xtc,ytc,7), \"but expected\", 3.015)\n",
    "print(linear_regression_gradient(thetatc,xtc,ytc,7), \"as expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1],\n",
       "       [ 2,  4,  8, 16],\n",
       "       [ 3,  9, 27, 81]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test case for polynomial features\n",
    "poly_features(np.array([1,2,3]),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.225     ,  0.22639379,  0.24181128,  0.22577375,  0.23375395,\n",
       "         0.42124037,  0.22619841,  1.99729238,  2.48294797,  3.31846602]),\n",
       " array([  1.225     ,   0.9195883 ,   0.35917267,   1.48273688,\n",
       "          2.17472704,   8.46294276,   1.54312743,  42.82424024,\n",
       "         48.99623317,  55.97614495]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test cases\n",
    "Xtc = add_bias_term(np.array([2,3,4,5]))\n",
    "Xvaltc = add_bias_term(np.array([7,-2]))\n",
    "ytc=np.array([7,6,4,5])\n",
    "yvaltc = np.array([2,12])\n",
    "validation_curve(Xtc,ytc,Xvaltc,yvaltc,lambdas)\n",
    "# results do not match expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below testcases... to move to a distinct python module and wrapped up with the assertion classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tc stands for testcase\n",
    "tcxval = np.array([  [-0.50000 ,  0.00000], [-0.40000  , 0.10000], [-0.30000,   0.20000],  \n",
    "  [-0.20000,   0.30000],  [-0.10000,   0.40000],  [-0.50000  , 0.00000],\n",
    "              [-0.40000 ,  0.10000],[-0.30000,   0.20000],[ -0.20000,   0.30000],[ -0.10000  , 0.40000]])\n",
    "tcxval = add_bias_term(tcxval)\n",
    "tcxval[:,0] = tcxval[:,0]/10.0\n",
    "tcx= np.array([[-5.0,   0.0],\n",
    "  [-4.0  , 1.0],\n",
    "  [-3.0 ,  2.0],\n",
    "  [-2.0 ,  3.0],\n",
    "  [-1.0 ,  4.0]])\n",
    "tcx = add_bias_term(tcx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcyval = np.array([ -0.20000,\n",
    "  -0.10000,\n",
    "   0.00000,\n",
    "   0.10000,\n",
    "   0.20000,\n",
    "  -0.20000,\n",
    "  -0.10000,\n",
    "   0.00000,\n",
    "   0.10000,\n",
    "   0.20000])\n",
    "tcy = np.array([\n",
    "  -2.0,\n",
    "  -1.0,\n",
    "   0.0,\n",
    "   1.0,\n",
    "   2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2, -0.1,  0. ,  0.1,  0.2, -0.2, -0.1,  0. ,  0.1,  0.2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcyval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tctrerr , tccverr = learning_curve(_X=tcx,_y=tcy,_Xval=tcxval,_yval=tcyval,_lda=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0029249   0.02647851  0.00056235  0.00069043  0.00246918]\n",
      "[  1.09759270e-02   4.39773921e-03   6.53419752e-06   6.23719258e-06\n",
      "   2.46917798e-05]\n"
     ]
    }
   ],
   "source": [
    "# Why all 0s???\n",
    "print(tctrerr)\n",
    "print(tccverr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
